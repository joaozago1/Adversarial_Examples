{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defensive Distillation Application\n",
    "\n",
    "In this notebook an application example of defensive distillation (Papernot, et at. 2016) method for preventing adversarial examples is presented. For deeper details check for the original paper of the method in [https://arxiv.org/pdf/1511.04508.pdf].\n",
    "\n",
    "**References**\n",
    "\n",
    "N. Papernot, P. McDaniel, X. Wu, S. Jha and A. Swami, \"Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks,\" 2016 IEEE Symposium on Security and Privacy (SP), San Jose, CA, 2016, pp. 582-597."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5847,
     "status": "ok",
     "timestamp": 1586626516717,
     "user": {
      "displayName": "João Gabriel Zago",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgPGbzyWQfikM6HpPseJYZrqgcAMjPg8PTAaPpBHw=s64",
      "userId": "08217544875067793805"
     },
     "user_tz": 180
    },
    "id": "Vi_8vyoYSWUO",
    "outputId": "5bafd8a7-f40a-431f-85ee-d2c0519d5dcf"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mspatches\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf #import the tensorflow library\n",
    "from tensorflow.keras import datasets, layers, models #import the keras library from tensorflow\n",
    "from tensorflow import keras\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E0diChIWSymf"
   },
   "outputs": [],
   "source": [
    "def convert_to_rgb(img, dim1, dim2):\n",
    "    img = cv2.resize(img, (dim1, dim2), interpolation=cv2.INTER_AREA) \n",
    "    img_rgb = np.asarray(np.dstack((img, img, img)), dtype=np.uint8)\n",
    "    \n",
    "    return img_rgb\n",
    "\n",
    "def convert_img_set_to_rgb(img_set, dim1, dim2):\n",
    "\n",
    "    rgb_img_list = list()\n",
    "    \n",
    "    for i in range(len(img_set)):\n",
    "        rgb = convert_to_rgb(img_set[i], dim1, dim2)\n",
    "        rgb_img_list.append(rgb)\n",
    "        \n",
    "    rgb_arr = np.stack([rgb_img_list],axis=4)\n",
    "    img_set = np.squeeze(rgb_arr, axis=4)\n",
    "    # print(img_set.shape)\n",
    "    \n",
    "    return img_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4180,
     "status": "ok",
     "timestamp": 1586626548332,
     "user": {
      "displayName": "João Gabriel Zago",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgPGbzyWQfikM6HpPseJYZrqgcAMjPg8PTAaPpBHw=s64",
      "userId": "08217544875067793805"
     },
     "user_tz": 180
    },
    "id": "sSY4OHC0T7PO",
    "outputId": "f8fe3c29-232c-498b-a49e-b3fd3789016d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
    "\n",
    "train_labels_list = list()\n",
    "\n",
    "for i in range(len(train_labels)):\n",
    "    aux_array = np.zeros(10)\n",
    "    aux_array[train_labels[i]] = 1\n",
    "    train_labels_list.append(aux_array)\n",
    "\n",
    "train_labels = np.asarray(train_labels_list)\n",
    "\n",
    "test_labels_list = list()\n",
    "\n",
    "for i in range(len(test_labels)):\n",
    "    aux_array = np.zeros(10)\n",
    "    aux_array[test_labels[i]] = 1\n",
    "    test_labels_list.append(aux_array)\n",
    "\n",
    "test_labels = np.asarray(test_labels_list)\n",
    "\n",
    "train_images_adjusted = convert_img_set_to_rgb(train_images, 32, 32)\n",
    "\n",
    "test_images_adjusted = convert_img_set_to_rgb(test_images, 32, 32)\n",
    "\n",
    "train_images_adjusted, test_images_adjusted = train_images_adjusted/255, test_images_adjusted/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XTUliyAwSMpv"
   },
   "outputs": [],
   "source": [
    "def conv2d(input_x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(input_x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    return tf.nn.bias_add(x, b)\n",
    "\n",
    "def act_relu(x):\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(input_x, k=2):\n",
    "    return tf.nn.max_pool(input_x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "def VGG_16_Temperature(input_x, weights, biases, temperature):\n",
    "    conv1 = conv2d(input_x, weights[0], biases[0])\n",
    "    conv1_relu = act_relu(conv1)\n",
    "    conv2 = conv2d(conv1_relu, weights[1], biases[1])\n",
    "    conv2_relu = act_relu(conv2)\n",
    "    # conv1_do = tf.nn.dropout(conv1_relu, rate=0.25)\n",
    "    conv2_max_pooling = maxpool2d(conv2_relu, k=2)\n",
    "\n",
    "    conv3 = conv2d(conv2_max_pooling, weights[2], biases[2])\n",
    "    conv3_relu = act_relu(conv3)\n",
    "    conv4 = conv2d(conv3_relu, weights[3], biases[3])\n",
    "    conv4_relu = act_relu(conv4)\n",
    "    # conv1_do = tf.nn.dropout(conv1_relu, rate=0.25)\n",
    "    conv4_max_pooling = maxpool2d(conv4_relu, k=2)\n",
    "\n",
    "    conv5 = conv2d(conv4_max_pooling, weights[4], biases[4])\n",
    "    conv5_relu = act_relu(conv5)\n",
    "    conv6 = conv2d(conv5_relu, weights[5], biases[5])\n",
    "    conv6_relu = act_relu(conv6)\n",
    "    conv7 = conv2d(conv6_relu, weights[6], biases[6])\n",
    "    conv7_relu = act_relu(conv7)\n",
    "    # conv1_do = tf.nn.dropout(conv1_relu, rate=0.25)\n",
    "    conv7_max_pooling = maxpool2d(conv7_relu, k=2)\n",
    "\n",
    "    conv8 = conv2d(conv7_max_pooling, weights[7], biases[7])\n",
    "    conv8_relu = act_relu(conv8)\n",
    "    conv9 = conv2d(conv8_relu, weights[8], biases[8])\n",
    "    conv9_relu = act_relu(conv9)\n",
    "    conv10 = conv2d(conv9_relu, weights[9], biases[9])\n",
    "    conv10_relu = act_relu(conv10)\n",
    "    # conv1_do = tf.nn.dropout(conv1_relu, rate=0.25)\n",
    "    conv10_max_pooling = maxpool2d(conv10_relu, k=2)\n",
    "\n",
    "    conv11 = conv2d(conv10_max_pooling, weights[10], biases[10])\n",
    "    conv11_relu = act_relu(conv11)\n",
    "    conv12 = conv2d(conv11_relu, weights[11], biases[11])\n",
    "    conv12_relu = act_relu(conv12)\n",
    "    conv13 = conv2d(conv12_relu, weights[12], biases[12])\n",
    "    conv13_relu = act_relu(conv13)\n",
    "    # conv1_do = tf.nn.dropout(conv1_relu, rate=0.25)\n",
    "    conv13_max_pooling = maxpool2d(conv13_relu, k=2)\n",
    "    \n",
    "    dens1 = tf.reshape(conv13_max_pooling, [-1, weights[13].get_shape().as_list()[0]])\n",
    "    dens1 = tf.add(tf.matmul(dens1, weights[13]), biases[13])\n",
    "    dens1_relu = tf.nn.relu(dens1)\n",
    "    \n",
    "    dens2 = tf.add(tf.matmul(dens1_relu, weights[14]), biases[14])\n",
    "    dens2_relu = tf.nn.relu(dens2)\n",
    "\n",
    "    dens3 = tf.add(tf.matmul(dens2_relu, weights[15]), biases[15])\n",
    "    \n",
    "    dens4 = tf.divide(dens3, temperature) #insert the temperature\n",
    "    return dens4\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None, 32, 32, 3))\n",
    "y = tf.placeholder(tf.float32, shape=(None, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KG0FmjidSZxU"
   },
   "outputs": [],
   "source": [
    "weights = [tf.Variable(tf.random_normal([3, 3, 3, 64], stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal([3, 3, 64, 64], stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal([3, 3, 128, 128], stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal([3, 3, 128, 256], stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal([3, 3, 256, 256], stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal([1, 1, 256, 256], stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal([3, 3, 256, 512], stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal([3, 3, 512, 512], stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal([1, 1, 512, 512], stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal([3, 3, 512, 512], stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal([3, 3, 512, 512], stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal([1, 1, 512, 512], stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal([512, 4096], stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal([4096, 4096], stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal([4096, 10], stddev=0.1))       \n",
    "]\n",
    "\n",
    "biases = [tf.Variable(tf.random_normal([64], stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal([64], stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal([128], stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal([128], stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal([256], stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal([256], stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal([256], stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal([512], stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal([512], stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal([512], stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal([512], stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal([512], stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal([512], stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal([4096], stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal([4096], stddev=0.1)),\n",
    "    tf.Variable(tf.random_normal([10]))      \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tm-CjblzTRsd"
   },
   "outputs": [],
   "source": [
    "def train_distilled_temperature(name_to_save, x, y, weights, biases, train_images, train_labels, test_images, test_labels, epochs, temperature):\n",
    "    logit = VGG_16_Temperature(x, weights, biases, temperature)\n",
    "\n",
    "    prediction = tf.nn.softmax(logit)\n",
    "\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=logit))\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=0.000005).minimize(loss_op)\n",
    "\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    batch_size = 150\n",
    "\n",
    "    model_weights_trained = name_to_save + \"_\" + str(temperature) + \"_VGG16_Weights\"\n",
    "    model_bias_trained = name_to_save + \"_\" + str(temperature) + \"_VGG16_Bias\"\n",
    "\n",
    "    acc_train_list = list()\n",
    "    acc_test_list = list()\n",
    "    loss_train_list = list()\n",
    "    loss_test_list = list()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # print(sess.run(weights['wc1']))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            print(\"EPOCH: \" + str(epoch+1))\n",
    "\n",
    "            for j in range(int(len(train_images)//batch_size)):\n",
    "                \n",
    "                image_batch = train_images[j*batch_size : min((j+1)*batch_size, len(train_images))]\n",
    "                \n",
    "                label_batch = train_labels[j*batch_size : min((j+1)*batch_size, len(train_labels))]\n",
    "\n",
    "                # print(sess.run(loss_op, feed_dict={x: image_batch, y: label_batch}))\n",
    "\n",
    "                sess.run(train_op, feed_dict={x: image_batch, y: label_batch})\n",
    "\n",
    "            # print(sess.run(weights['wc1']))\n",
    "\n",
    "            acc_train = 0\n",
    "            loss_train = 0\n",
    "\n",
    "            for j in range(10):\n",
    "\n",
    "                image_batch = train_images[j*len(train_images)//10 : min((j+1)*len(train_images)//10, len(train_images))]\n",
    "                label_batch = train_labels[j*len(train_labels)//10 : min((j+1)*len(train_labels)//10, len(train_labels))]\n",
    "\n",
    "                loss_train += sess.run(loss_op, feed_dict={x: image_batch, y: label_batch})\n",
    "                acc_train += sess.run(accuracy, feed_dict={x: image_batch, y: label_batch})\n",
    "            \n",
    "            loss_test = sess.run(loss_op, feed_dict={x: test_images, y: test_labels})\n",
    "            acc_test = sess.run(accuracy, feed_dict={x: test_images, y: test_labels})\n",
    "\n",
    "            acc_train_list.append(acc_train/10)\n",
    "            acc_test_list.append(acc_test)\n",
    "            loss_train_list.append(loss_train/10)\n",
    "            loss_test_list.append(loss_test)\n",
    "\n",
    "            print(\" train loss: \" + str(loss_train/10) + \" train acc: \" + str(acc_train/10) + \" test loss: \" + str(loss_test) + \" test acc: \" + str(acc_test))\n",
    "\n",
    "        weights_after_train = sess.run(weights)\n",
    "        biases_after_train = sess.run(biases)\n",
    "\n",
    "    np.save(model_weights_trained + \"_\" + str(temperature) + \".npy\", weights_after_train)\n",
    "    np.save(model_bias_trained + \"_\" + str(temperature) + \".npy\", biases_after_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ejei7VzFT2-J"
   },
   "outputs": [],
   "source": [
    "def open_professor_weights_temperature(professor_file_name, temperature):\n",
    "    weights_trained = np.load(professor_file_name + '_' + str(temperature) + '_VGG16_Weights.npy', allow_pickle=True).tolist()\n",
    "    for i in range(len(weights_trained)):\n",
    "        weights_trained[i] = tf.Variable(weights_trained[i])\n",
    "    bias_trained = np.load(professor_file_name + '_' + str(temperature) + \"_VGG16_Bias.npy\", allow_pickle=True).tolist()\n",
    "    for i in range(len(bias_trained)):\n",
    "        bias_trained[i] = tf.Variable(bias_trained[i])\n",
    "    return weights_trained, bias_trained\n",
    "\n",
    "def get_professor_labels(professor_file_name, input_train_images, temperature):\n",
    "    \n",
    "    weights_trained, bias_trained = open_professor_weights_temperature(professor_file_name, temperature)\n",
    "\n",
    "    logit = VGG_16_Temperature(x, weights_trained, bias_trained, temperature)\n",
    "\n",
    "    prediction = tf.nn.softmax(logit)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for j in range(10):\n",
    "            image_batch = input_train_images[j*len(input_train_images)//10 : min((j+1)*len(input_train_images)//10, len(input_train_images))]\n",
    "            if j == 0:\n",
    "                test_labels_professor = sess.run(prediction, feed_dict={x: image_batch})\n",
    "            else:\n",
    "                test_labels_professor = np.append(np.copy(test_labels_professor), sess.run(prediction, feed_dict={x: image_batch}), axis=0)\n",
    "        # print(sess.run(accuracy, feed_dict={x: test_images_adjusted, y: test_labels}))\n",
    "    return test_labels_professor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_distilled_temperature(\"Original\", x, y, weights, biases, train_images_adjusted, train_labels, test_images_adjusted, test_labels, 50, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HH6yz0VnUL-B"
   },
   "outputs": [],
   "source": [
    "test_labels_professor = get_professor_labels(\"Original\", train_images_adjusted, 2000)\n",
    "\n",
    "train_distilled_temperature(\"Original_Distilled\", x, y, weights, biases, train_images_adjusted, test_labels_professor, test_images_adjusted, test_labels, 50, 2000)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO0GFtNohg3gKT2S4XCCKRr",
   "collapsed_sections": [],
   "name": "Defensive_Distillation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
